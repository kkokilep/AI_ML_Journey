# General Notes


These notes are taken from: this [Andrew Karpathy Video](https://www.youtube.com/watch?v=kCc8FmEb1nY&list=PLAqhIrjkxbuWI23v9cThsA9GvCAUhRvKZ&index=7).
# <u> Date 10/01/2025 </u>

## Topic: Notes from Let's Build GPT from Scratch
#### Notes: 

* GPT  = General Pretrained Transformer
* Train a character level language model
    * Train with tiny shakespeare dataset

* Tokenize means convert characters or substrings into integers
* In this work, the characters are being mapped from 1-65 based on the number of individual characters.

* Real tokenization uses strategies to map longer substrings to specific integers.

* Training involves taking a block size to train the model.
* Idea is to predict the next token in the sequence and train with a loss to penalize mis predictions.
* Need batches with text that are processed independently of each other.
